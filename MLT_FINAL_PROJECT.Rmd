---
title: "MLT_Final_Project"
author: "Alexander Shemetev"
date: "13 8 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The first part of the Project: data primary reading

This is an R Markdown document. It is made in **Knitr** package

```{r summary_1}
# url1 = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# url2 = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# Too long to download from URL directly
url1 = "C:/Users/Alex/Documents/COURSERA STUDIES/Practical Machine Learning/Final_Project/pml-training.csv"
url2 = "C:/Users/Alex/Documents/COURSERA STUDIES/Practical Machine Learning/Final_Project/pml-testing.csv"

training = read.csv(paste0(url1),na.strings=c("NA","#DIV/0!",""))
testing = read.csv(paste0(url2),na.strings=c("NA","#DIV/0!",""))
dim(training)
dim(testing)
summary(training)
```

## The first part of the Project: data primary analysis

Done:

```{r pressure, echo=FALSE}
summary(training)
str(training)
head(training, 10)

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code.


## Cross-Validation

It is decided to use 60/40 separation between the training and testing datasets
Since testing dataset is too small - we will create a new testing dataset from partitioning the training dataset that has sufficient number of observations

```{r cross_validation_1, warning=FALSE, message=FALSE, echo=TRUE}
# loading the packages
install_and_load = function(name, char = T)
{
  if (!require(name, character.only = char)) 
  {
    install.packages(name)
  }
  require(name, character.only = char)
}
sapply(
  c("caret","randomForest","magrittr", "tidyr", "rattle"),
  install_and_load
)
rm(install_and_load)

# Training dataset (60%); Testing dataset (40%) 
set.seed(13082020)
inTrain = caret::createDataPartition(y=training$classe,p=0.6, list=FALSE)
# training dataset - select rows
training_set = training[inTrain,]
# testing dataset - select remaining rows
testing_set = training[-inTrain,]

```

## Data cleaning 

Near zero covariates and the data containing too many NAs or NANs or Inf or -Inf should be removed from the DS.

```{r cleaning_data, warning=FALSE, message=FALSE, echo=TRUE}

# Removing near zero covariates
my_near_zero_vars = nearZeroVar(x = training, saveMetrics = TRUE)
# View(my_near_zero_vars) #DF of covariates and if they are NZV

# Removing covariates with more than 40% NA values
my_NA_values = !as.logical(apply(training, 2, function(x){ mean(is.na(x)) >= 0.4}))
# View(my_NA_values) # Returns TRUE/FALSE DF for each column if NAs are above SET percent.

# Cleaning data
my_indicator_up_2 = my_NA_values*1 + (!my_near_zero_vars$nzv)*1
# View(my_indicator_up_2) #Returns DF with 0, 1 or 2 for each column;
# 2 means we have NAs below the threshold and not NZV covariate
my_indicator_up_2_equal_2 = my_indicator_up_2 == 2
# View(my_indicator_up_2_equal_2) # Now we have a list of all indicators equal to 2 TRUE/FALSE for each column
sum(my_indicator_up_2_equal_2)
# Shows the number of usefulcovariates (columns)

#View(data.frame(my_NA_values, !my_near_zero_vars$nzv, my_indicator_up_2, my_indicator_up_2_equal_2))

training_set = training_set[,my_indicator_up_2_equal_2] #View(training_set) #OK variables left
testing_set = testing_set[, my_indicator_up_2_equal_2]

# The X colymn (the first column) is something like a database key - remove it:
training_set = training_set[, -1] #Now it is a colum row.names
testing_set = testing_set[, -1]
# Same operations with the primary testing datatset
testing = testing[,my_indicator_up_2_equal_2]
testing = testing[,-1]

# This loop is inspired by the Coursera courses and additional searching
# We had the testing DF uploaded from http - it has about 20 observations
# Coerce the data into the same type in order to avoid the "Matching Error" when calling random forest model, due to different levels in variables
# This loop helps to apply the model MLT to any further DS without the mentioned error
for (i in 1:length(testing) ) {
  for(j in 1:length(training_set)) {
    if( length( grep(names(training_set[i]), names(testing)[j]) ) == 1)  {
      class(testing[j]) <- class(training_set[i])
    }      
  }      
}

# To get the same class between testing and training_set
testing = testing[,-ncol(testing)]
testing <- rbind(training_set[2, -58] , testing) #It adds additional observation (row) - we may see if it is added correctly
testing <- testing[-1,]

#Now the DS is ready for work
```


## RPART method prediction with decision trees 

```{r predictive_trees, warning=FALSE, message=FALSE, echo=TRUE}
# Build model
set.seed(13082020)
# training_set[,-ncol(training_set)] #Means without the shadow column row.names generated by R automatically. dim(training_set) [1] 11776    58 dim(training_set[,-ncol(training_set)]) [1] 11776    57


my_tree = train(y = training_set$classe,
                 x = training_set[,-ncol(training_set)],
                 method = "rpart")

# Plot classification tree
rattle::fancyRpartPlot(
  my_tree$finalModel
)

# Predictions with rpart model
prediction_from_tree = predict(my_tree, testing_set[,-ncol(testing_set)])
prediction_from_tree = predict(my_tree, testing_set)
# View(prediction_from_tree)
# View(testing_set$classe)
str(prediction_from_tree) #Factor
str(testing_set$classe) #Character - will cause error in confusion matrix
# solution:
reality_without_trees = as.factor(testing_set$classe)
str(reality_without_trees)

# Get results (Accuracy, etc.)
# confusionMatrix(prediction_from_tree, testing_set$classe)

confusionMatrix(prediction_from_tree, reality_without_trees)
# Accuracy is 55% which is OK for micro-data



```



## RPART method prediction with Random Forest

```{r random_forest, warning=FALSE, message=FALSE, echo=TRUE}

# Prediction with Random Forest
# Build model
set.seed(13082020)
str(testing_set$classe) #Character - will cause error in Random forest (from documentation)
testing_set$classe <- factor(testing_set$classe) 
str(testing_set$classe)
training_set$classe <- factor(training_set$classe) 
str(training_set$classe)

my_random_forest_model = randomForest(
  classe ~ .,
  data = training_set,
  ntree = 150) #No need to overload system with higher Ntrees

# Plot the Random Forests model
plot(my_random_forest_model) #Chart reveals - no urgent need to plant more trees than we have already planted

# Predict with random forest model
RF_prediction = predict(
  my_random_forest_model,
  testing_set[,-ncol(testing_set)]
)

# Get results (Accuracy, etc.)
confusionMatrix(RF_prediction, testing_set$classe)
# Acuracy is almost 100% - it is even better

```


## First conclusion

Random forest mode has accuracy level at almost 100% - it is even better than our decision trees model we used as well. 


## Using our model to predict the testing dataset



```{r pml-testing predictions, warning=FALSE, message=FALSE, echo=TRUE}

# Get predictions for the 20 observations of the original pml-testing.csv

my_prediction = predict(my_random_forest_model, testing)
my_prediction
summary(my_prediction)
```


## Last action: saving the results

```{r saving results, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}
# Saving predictions for testing dataset
testing$predicted_classe = my_prediction

write.table(
  testing,
  file = "C:/Users/Alex/Documents/COURSERA STUDIES/Practical Machine Learning/Final_Project/testing_and_predictions",
  quote = F
)

```


Thank you







